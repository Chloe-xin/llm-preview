{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3276edeb",
   "metadata": {},
   "source": [
    "# Chapter-2 大模型使用\n",
    "\n",
    "本节学习大纲\n",
    "\n",
    "- 掌握通过 API 调用云端大模型的基本方法。\n",
    "- 掌握使用 transformers 库在本地运行大模型的基本流程。\n",
    "- 了解 vLLM 等高性能推理框架的用途和基本使用方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89e5ec",
   "metadata": {},
   "source": [
    "上一节课我们主要解了什么是大模型，以及在哪里可以找到它们，比如 Hugging Face Hub 以及 ModelScope。本节的主要目标是“动手”，从理论走向实践，真正地把大模型用起来。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e262d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "使用大模型主要有两种路径：\n",
    "\n",
    "1. 云端API调用：像使用一个在线服务一样，我们发送请求，远端的服务器（云）返回结果。这种方式简单、方便，对我们自己的电脑配置要求不高。\n",
    "\n",
    "2. 本地化部署：把大模型下载到我们自己的电脑（服务器）上并运行起来。这种方式给了我们最大的控制权和隐私保障，但对硬件（尤其是显卡）有一定要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ff506",
   "metadata": {},
   "source": [
    "## 2.1 云端大模型调用：最便捷的路径\n",
    "\n",
    "这是使用大模型最直接的方式。我们不需要关心硬件配置、模型加载等复杂问题，只需要获得一个许可（API Key），然后按照服务商提供的地址和格式发送请求即可。\n",
    "\n",
    "我们将以[硅基流动（SiliconFlow）](https://cloud.siliconflow.cn/i/ybUFvmqK)平台为例，它提供了与 OpenAI API 完全兼容的接口，我们可以直接使用 OpenAI 中的 Python 库调用模型。\n",
    "\n",
    "> 注：虽然我们使用 `openai` 库的方式来调用云端大模型，但其本质是向远程服务器发送请求，所以同学们也可以使用 python 中的 `requests` 库，以发送网络请求的方式也可以实现云端大模型的调用。学有余力的同学可以自行探索一下～"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbf606",
   "metadata": {},
   "source": [
    "### Step 1：获取 [硅基流动](https://cloud.siliconflow.cn/i/ybUFvmqK) API Key\n",
    "\n",
    "访问硅基流动平台官网，注册账号，在个人中心找到你的 API Key（密钥） 和 API Base URL（接口地址）。请务必保管好你的 API Key，不要泄露。\n",
    "\n",
    "如下图所示，创建好账户之后，即可获取到 API Key 和 API Base URL。\n",
    "\n",
    "![](./images/2-1%20硅基流动api-key创建.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330cee35",
   "metadata": {},
   "source": [
    "### Step 2: 调用大模型\n",
    "\n",
    "接下来，我们安装`openai`库，用于调用大模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b882b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d90054",
   "metadata": {},
   "source": [
    "使用以下代码调用云端API大模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576f273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "你好！😊 有什么我可以帮你的吗？无论是解答问题、提供建议，还是只是闲聊，我都很乐意和你交流！\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"your api key\", \n",
    "                base_url=\"https://api.siliconflow.cn/v1\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen3-8B\",\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': \"你好！\"}\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    stream=False\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8e29ef",
   "metadata": {},
   "source": [
    "- temperature（温度）\n",
    "  - 控制输出的随机性/创造性\n",
    "  - 取值范围 0-1，值越大随机性越强\n",
    "  - 0表示始终选择最可能的词\n",
    "\n",
    "- max_tokens（最大令牌数）\n",
    "  - 限制模型回复的最大长度\n",
    "  - 1 token 约等于 4 个字符\n",
    "  - 需要根据具体任务设置合适的值\n",
    "\n",
    "- stream\n",
    "  - 是否流式输出\n",
    "  - 取值范围 True/False\n",
    "  - True 表示流式输出，模型会边生成边输出\n",
    "  - False 表示一次性输出全部结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34b964",
   "metadata": {},
   "source": [
    "我们把以上代码封装为一个函数，可以接受用户输入，历史对话和系统提示词，最终返回模型生成的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf562f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "你好呀！今天过得怎么样呀？有什么我可以帮你的吗？😊\n"
     ]
    }
   ],
   "source": [
    "def chat_with_model(user_input: str, history: list = None, temperature: float = 0.7, system_prompt: str = None) -> str:\n",
    "    \"\"\"\n",
    "    与大模型进行对话的函数\n",
    "    \n",
    "    Args:\n",
    "        user_input: 用户输入的文本\n",
    "        history: 历史对话记录列表\n",
    "        temperature: 温度参数，控制输出的随机性\n",
    "        system_prompt: 系统提示词\n",
    "    \n",
    "    Returns:\n",
    "        str: 模型返回的文本\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化 OpenAI 客户端\n",
    "    client = OpenAI(api_key=\"your api key\", \n",
    "                base_url=\"https://api.siliconflow.cn/v1\")\n",
    "\n",
    "    # 初始化历史记录\n",
    "    if history is None:\n",
    "        history = []\n",
    "        \n",
    "    # 构建消息列表\n",
    "    messages = []\n",
    "    \n",
    "    # 添加系统提示词(如果有)\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    # 添加历史对话\n",
    "    for msg in history:\n",
    "        messages.append(msg)\n",
    "    \n",
    "    # 添加当前用户输入\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # 调用API获取响应\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-8B\",\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # 返回模型回复的文本\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "print(chat_with_model(\"你好哇\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcfee3a",
   "metadata": {},
   "source": [
    "## 2.2 本地部署与调用：更强的掌控力\n",
    "\n",
    "调用云端的大模型固然很方便，但它存在一些问题。比如：数据的隐私性。当同学们对外发送请求的时候，本地数据已经泄漏给外界了。所以当我们用一些特殊需求时，往往会采用本地部署大模型然后进行调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12033d60",
   "metadata": {},
   "source": [
    "### 2.2.1 使用 `transformers` 部署大模型\n",
    "\n",
    "`transformers` 是 Hugging Face 公司开发的一个库，是当下进行NLP（自然语言处理）和模型研究开发的最流行的一个python库。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c98029",
   "metadata": {},
   "source": [
    "> 注：不知道大家有没有完成上一节布置的课后作业？（下载 Qwen3-4B 模型）\n",
    "\n",
    "没有下载好的同学可以运行以下代码，在 ModelScope 平台下载模型。模型大小在 8G 左右。\n",
    "\n",
    "> 注：如果模型下载的环境也没有配置完全，可以再阅读以下第一节的内容，进行环境配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50069c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "\n",
    "model_dir = snapshot_download('Qwen/Qwen3-4B', cache_dir='/home/mw/model', revision='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e786dc6",
   "metadata": {},
   "source": [
    "模型下载完成之后，就可以使用 `transformers` 库加载模型了，首先我们需要安装一些加载模型的环境配置，如果同学们在第一节安装过了，那就不需要再安装了。\n",
    "\n",
    "```python\n",
    "!pip install -q modelscope transformers accelerate\n",
    "```\n",
    "\n",
    "> 注：没有配置环境的同学，请复制以上代码到自己的环境中运行安装。\n",
    "\n",
    "配置好环境的同学，可以使用以下代码运行加载模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c038fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 设置模型本地路径\n",
    "model_name = \"/home/mw/model/Qwen/Qwen3-4B\"\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",  # 自动选择合适的数据类型\n",
    "    device_map=\"cuda:0\",    # 自动选择可用设备(CPU/GPU)\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 准备模型输入\n",
    "prompt = \"你好，请介绍一下自己\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # 选择是否打开深度推理模式\n",
    ")\n",
    "# 将输入文本转换为模型可处理的张量格式\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 生成文本\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768  # 设置最大生成token数量\n",
    ")\n",
    "# 提取新生成的token ID\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# 解析思考内容\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    # 查找结束标记\"</think>\"的位置\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "# 解码思考内容和最终回答\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "# 打印结果\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70decac1",
   "metadata": {},
   "source": [
    "同样我们也可以将其封装为一个函数，如下代码所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e06cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_local_model(model_path: str, user_input: str, history: list = None, temperature: float = 0.7, max_new_tokens: int = 1024, enable_thinking: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    使用本地大模型进行对话的函数\n",
    "    \n",
    "    Args:\n",
    "        model_path: 模型本地路径\n",
    "        user_input: 用户输入的文本\n",
    "        history: 历史对话记录列表，格式为 [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]\n",
    "        temperature: 温度参数，控制输出的随机性 (0.0-1.0)\n",
    "        max_new_tokens: 最大生成token数量\n",
    "        enable_thinking: 是否启用深度推理模式\n",
    "    \n",
    "    Returns:\n",
    "        dict: 包含思考内容和最终回答的字典 {\"thinking_content\": str, \"content\": str}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 加载分词器和模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=\"auto\",  # 自动选择合适的数据类型\n",
    "        device_map=\"cuda:0\",    # 自动选择可用设备(CPU/GPU)\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 初始化历史记录\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # 构建消息列表\n",
    "    messages = []\n",
    "    \n",
    "    # 添加历史对话\n",
    "    for msg in history:\n",
    "        messages.append(msg)\n",
    "    \n",
    "    # 添加当前用户输入\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # 应用聊天模板\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking  # 选择是否打开深度推理模式\n",
    "    )\n",
    "    \n",
    "    # 将输入文本转换为模型可处理的张量格式\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 生成文本\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens,  # 设置最大生成token数量\n",
    "        temperature=temperature,\n",
    "        do_sample=True if temperature > 0 else False  # 当temperature > 0时启用采样\n",
    "    )\n",
    "    \n",
    "    # 提取新生成的token ID\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "    \n",
    "    # 解析思考内容\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        # 查找结束标记\"</think>\"的位置\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    # 解码思考内容和最终回答\n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"thinking_content\": thinking_content,\n",
    "        \"content\": content\n",
    "    }\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置模型路径\n",
    "    model_path = \"/home/mw/model/Qwen/Qwen3-4B\"\n",
    "    \n",
    "    # 单轮对话示例\n",
    "    result = chat_with_local_model(\n",
    "        model_path=model_path,\n",
    "        user_input=\"你好，请介绍一下自己\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"thinking content:\", result[\"thinking_content\"])\n",
    "    print(\"content:\", result[\"content\"])\n",
    "    \n",
    "    # 多轮对话示例\n",
    "    history = [\n",
    "        {\"role\": \"user\", \"content\": \"你好\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"你好！我是Qwen，一个AI助手。\"}\n",
    "    ]\n",
    "    \n",
    "    result = chat_with_local_model(\n",
    "        model_path=model_path,\n",
    "        user_input=\"你能帮我写一首诗吗？\",\n",
    "        history=history,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    print(\"\\n多轮对话结果:\")\n",
    "    print(\"thinking content:\", result[\"thinking_content\"])\n",
    "    print(\"content:\", result[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cb8d3",
   "metadata": {},
   "source": [
    "虽然使用 transformers 在本地部署模型能让我们获得完整的控制权限，但这种方式存在一定的性能瓶颈，尤其在首次推理时表现明显。这种本地部署方式更适合进行简单模型加载测试或算法研究工作，但若要将模型打造成一个支持高并发调用的服务，其计算效率则显得捉襟见肘。这便是 transformers 本地部署方案在性能方面的主要局限。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7064dc",
   "metadata": {},
   "source": [
    "### 2.2.2 进阶：使用 vLLM 进行高性能部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450f444",
   "metadata": {},
   "source": [
    "为了解决上述效率问题，我们可以使用像 vLLM 这样的高性能推理框架。vLLM 通过 `PagedAttention` 等先进的内存管理技术，极大地提升了模型的吞吐量（每秒能处理的请求数）和响应速度，特别是在并发请求的场景下。\n",
    "\n",
    "首先第一步，安装 vllm，直接安装即可～\n",
    "\n",
    "```python\n",
    "!pip install vllm\n",
    "```\n",
    "\n",
    "> 注意：安装时间较长，请耐心等待。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840050fc",
   "metadata": {},
   "source": [
    "使用以下命令在命令行终端启动 vllm 服务：\n",
    "\n",
    "```bash\n",
    "vllm serve /home/mw/model/Qwen/Qwen3-4B \\    \n",
    "  --served-model-name Qwen3-4B \\    \n",
    "  --max_model_len 1024 \\    \n",
    "  --enable-reasoning \\    \n",
    "  --reasoning-parser deepseek_r1\n",
    "```\n",
    "\n",
    "以上命令中各参数的含义如下：\n",
    "\n",
    "- `/home/mw/model/Qwen/Qwen3-4B`: 模型路径，指向本地存储的 Qwen 3.0 6B 模型文件\n",
    "- `--served-model-name Qwen3-4B`: 服务启动后使用的模型名称\n",
    "- `--max_model_len 1024`: 设置模型最大处理的序列长度为 1024 个 token\n",
    "- `--enable-reasoning`: 启用推理功能\n",
    "- `--reasoning-parser deepseek_r1`: 使用 deepseek_r1 作为推理解析器\n",
    "\n",
    "这些参数配置了 vllm 服务的基本运行参数，包括模型位置、服务名称、序列长度限制以及推理相关的功能设置。\n",
    "\n",
    "> 注意：如果想要对 vllm 服务进行更多定制化配置，建议参考 vllm 官方文档。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0650a5",
   "metadata": {},
   "source": [
    "然后就可以像使用 2.1 云端大模型一样的方式来调用 vllm 启动的模型服务，如下代码所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ca20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"xxx\", \n",
    "                base_url=\"http://127.0.0.1:8000/v1\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen3-4B\",\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': \"你好！\"}\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    stream=False\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540ab6e",
   "metadata": {},
   "source": [
    "## 课后作业\n",
    "\n",
    "请同学们尝试使用 python 中的 requests 来调用模型服务。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
